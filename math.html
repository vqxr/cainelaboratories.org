<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>BBT ‚Äî Mathematics</title>
<link rel="stylesheet" href="style.css">
<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      displayMath: [['$$','$$'], ['\\[','\\]']],
      tags: 'ams',
      macros: {
        R: '\\mathbb{R}',
        E: '\\mathbb{E}',
        argmax: '\\operatorname{argmax}',
        argmin: '\\operatorname{argmin}',
        softmax: '\\operatorname{softmax}',
        sigmoid: '\\operatorname{sigmoid}',
        logit: '\\operatorname{logit}'
      }
    },
    options: { skipHtmlTags: ['script','noscript','style','textarea','pre'] }
  };
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-chtml.min.js" async></script>
</head>
<body>

<div class="layout">

  <!-- SIDEBAR -->
<div class="sidebar-logo">
  <img src="WhatsApp Image 2026-02-26 at 19.38.22.jpeg" alt="BBT Logo" class="logo-image">
  <div class="name">Bloomsbury Burger<br>Therapeutics plc</div>
  <div class="sub">pre-seed ¬∑ pre-revenue ¬∑ pre-sanity</div>
</div>
    <div class="nav-section">
      <div class="nav-label">Pages</div>
      <a href="index.html" class="nav-item"><span class="icon">üè†</span> Overview</a>
      <a href="pipeline.html" class="nav-item"><span class="icon">‚öôÔ∏è</span> Pipeline</a>
      <a href="math.html" class="nav-item active"><span class="icon">‚àë</span> Mathematics</a>
      <a href="dataset.html" class="nav-item"><span class="icon">üóÑÔ∏è</span> Dataset</a>
      <a href="console.html" class="nav-item"><span class="icon">üñ•</span> Lab Console</a>
    </div>
    <div class="sidebar-divider"></div>
    <div class="nav-section">
      <div class="nav-label">On this page</div>
      <a href="#problem" class="nav-item"><span class="icon">¬∑</span> Problem formulation</a>
      <a href="#input" class="nav-item"><span class="icon">¬∑</span> Input representation</a>
      <a href="#selection" class="nav-item"><span class="icon">¬∑</span> Learnable selection</a>
      <a href="#and-gate" class="nav-item"><span class="icon">¬∑</span> Differentiable AND</a>
      <a href="#log-space" class="nav-item"><span class="icon">¬∑</span> Log-space stability</a>
      <a href="#loss" class="nav-item"><span class="icon">¬∑</span> Loss function</a>
      <a href="#cardinality" class="nav-item"><span class="icon">¬∑</span> Cardinality constraint</a>
      <a href="#inference" class="nav-item"><span class="icon">¬∑</span> Hard inference</a>
    </div>
  </nav>

  <!-- MAIN -->
  <main class="main">

    <!-- HERO -->
    <div class="hero">
      <div class="hero-badge">Theoretical Framework ¬∑ Differentiable Combinatorial Optimisation</div>
      <h1>The <span>Mathematics</span></h1>
      <p class="hero-desc">
        A complete derivation of our differentiable dual-marker optimisation framework.
        This is not a classifier, not a representation learner ‚Äî this is differentiable 
        Boolean subset optimisation over binary gene expression.
      </p>
    </div>

    <!-- SECTION 1: PROBLEM FORMULATION -->
    <section id="problem" class="reveal">
      <div class="section-header">
        <div class="section-icon">üìê</div>
        <h2>1. Problem formulation</h2>
      </div>
      <div class="callout purple" style="margin-bottom:20px;">
        <p>We want to find a pair of surface markers $(g_1, g_2)$ that maximises the probability of AND-gate activation in lesion cells while minimising activation in healthy cells. This is a discrete subset selection problem that we relax into a continuous, differentiable form.</p>
      </div>

      <div class="eq-item">
        <div class="eq-header">
          <span class="eq-title">Objective</span>
          <span class="eq-tag">Combinatorial search</span>
        </div>
        <div class="eq-formula">
          $$\argmax_{|S|=2} \left[ P(\text{AND active in lesion}) - P(\text{AND active in healthy}) \right]$$
        </div>
        <div class="eq-desc">
          where $S \subset \mathcal{G}$ is a set of two genes. Brute-force enumeration over $\binom{|\mathcal{G}|}{2} \approx 5.4 \times 10^8$ pairs is infeasible. We need a differentiable relaxation.
        </div>
      </div>
    </section>

    <!-- SECTION 2: INPUT REPRESENTATION -->
    <section id="input" class="reveal">
      <div class="section-header">
        <div class="section-icon">üî¢</div>
        <h2>2. Input representation</h2>
      </div>

      <div class="eq-item">
        <div class="eq-header">
          <span class="eq-title">Binary expression matrix</span>
          <span class="eq-tag">After scVI denoising</span>
        </div>
        <div class="eq-formula">
          $$X \in \{0, 1\}^{N \times G}$$
        </div>
        <div class="eq-desc">
          <p>$N$ = number of cells, $G = |\mathcal{G}|$ = number of genes.</p>
          <p>$X_{ij} = 1$ if gene $j$ is expressed in cell $i$, $0$ otherwise.</p>
          <p>We obtain this by denoising raw counts with scVI-VAE, then binarising the posterior expression estimates. This removes technical dropout while preserving biological signal.</p>
        </div>
      </div>
    </section>

    <!-- SECTION 3: LEARNABLE SELECTION LOGITS -->
    <section id="selection" class="reveal">
      <div class="section-header">
        <div class="section-icon">‚öôÔ∏è</div>
        <h2>3. Learnable selection logits</h2>
      </div>

      <div class="eq-item">
        <div class="eq-header">
          <span class="eq-title">Selection parameters</span>
          <span class="eq-tag">Continuous relaxation</span>
        </div>
        <div class="eq-formula">
          $$\alpha_j \in \mathbb{R} \quad \text{for each gene } j$$
        </div>
        <div class="eq-desc">
          Instead of discrete selection, we learn a continuous logit $\alpha_j$ for each gene. During training, these determine the soft selection weight.
        </div>
      </div>

      <div class="eq-item" style="margin-top:8px;">
        <div class="eq-header">
          <span class="eq-title">Gumbel-Softmax mask</span>
          <span class="eq-tag">Differentiable relaxation</span>
        </div>
        <div class="eq-formula">
          $$m_j = \sigma\left(\frac{\alpha_j + g_j}{\tau}\right)$$
        </div>
        <div class="eq-desc">
          <p>$g_j \sim \text{Gumbel}(0,1)$ adds noise for stochastic relaxation.</p>
          <p>$\tau$ is the temperature parameter:</p>
          <ul style="margin-top:4px; margin-left:20px;">
            <li>High $\tau$ ‚Üí smooth, uniform selection (exploration)</li>
            <li>Low $\tau$ ‚Üí nearly binary selection (exploitation)</li>
          </ul>
          <p>$m_j \in (0,1)$ is the soft selection weight for gene $j$.</p>
        </div>
      </div>
    </section>

    <!-- SECTION 4: DIFFERENTIABLE AND GATE -->
    <section id="and-gate" class="reveal">
      <div class="section-header">
        <div class="section-icon">üîå</div>
        <h2>4. Differentiable AND gate</h2>
      </div>

      <div class="callout blue" style="margin-bottom:20px;">
        <p>For a dual-target CAR to activate, <strong>both</strong> selected markers must be present on the cell surface. This is a logical AND operation. We need a differentiable version that works with soft selections.</p>
      </div>

      <div class="eq-item">
        <div class="eq-header">
          <span class="eq-title">Differentiable AND formulation</span>
          <span class="eq-tag">Key insight</span>
        </div>
        <div class="eq-formula">
          $$a_i = \prod_{j} (1 - m_j(1 - X_{ij}))$$
        </div>
        <div class="eq-desc">
          <p><strong>Why this works:</strong></p>
          <ul style="margin-top:4px; margin-left:20px;">
            <li>If gene $j$ is selected ($m_j \approx 1$) and absent in cell $i$ ($X_{ij} = 0$): term becomes $0$ ‚Üí whole product $0$</li>
            <li>If gene $j$ is selected ($m_j \approx 1$) and present ($X_{ij} = 1$): term becomes $1$</li>
            <li>If any selected gene is absent ‚Üí activation $a_i = 0$</li>
            <li>If all selected genes are present ‚Üí activation $a_i \approx 1$</li>
          </ul>
          <p>This exactly implements logical AND, but because $m_j$ is soft during training, gradients flow through the entire expression.</p>
        </div>
      </div>

      <div class="eq-item" style="margin-top:8px;">
        <div class="eq-header">
          <span class="eq-title">Binary input simplification</span>
          <span class="eq-tag">$X_{ij} \in \{0,1\}$</span>
        </div>
        <div class="eq-formula">
          $$a_i = \prod_{j: X_{ij}=0} (1 - m_j)$$
        </div>
        <div class="eq-desc">
          Since $X_{ij}$ is binary, the product simplifies beautifully: activation depends only on <strong>selected genes that are absent</strong>. Present genes contribute nothing (multiply by 1).
        </div>
      </div>
    </section>

    <!-- SECTION 5: LOG-SPACE STABILITY -->
    <section id="log-space" class="reveal">
      <div class="section-header">
        <div class="section-icon">üìä</div>
        <h2>5. Log-space for numerical stability</h2>
      </div>

      <div class="callout green" style="margin-bottom:20px;">
        <p>Products over many terms are numerically unstable ‚Äî they can underflow to zero. Converting to log-space turns the product into a sum, which is much more stable for gradient optimisation.</p>
      </div>

      <div class="eq-item">
        <div class="eq-header">
          <span class="eq-title">Log activation</span>
          <span class="eq-tag">Stable computation</span>
        </div>
        <div class="eq-formula">
          $$\log a_i = \sum_j \log(1 - m_j(1 - X_{ij}))$$
        </div>
      </div>

      <div class="eq-item" style="margin-top:8px;">
        <div class="eq-header">
          <span class="eq-title">Binary input simplification (log form)</span>
          <span class="eq-tag">Clean interpretation</span>
        </div>
        <div class="eq-formula">
          $$\log a_i = \sum_{j: X_{ij}=0} \log(1 - m_j)$$
        </div>
        <div class="eq-desc">
          <p>Even cleaner: $\log a_i = \sum_j (1 - X_{ij}) \log(1 - m_j)$</p>
          <p><strong>Interpretation:</strong> Sum of penalties for missing required markers.</p>
          <ul style="margin-top:4px; margin-left:20px;">
            <li>If gene present ($X_{ij}=1$) ‚Üí contributes $0$</li>
            <li>If gene absent ($X_{ij}=0$) ‚Üí contributes $\log(1 - m_j)$</li>
            <li>If $m_j \to 1$ and gene absent ‚Üí large negative penalty</li>
          </ul>
        </div>
      </div>

      <div class="eq-item" style="margin-top:8px;">
        <div class="eq-header">
          <span class="eq-title">Recovering activation</span>
        </div>
        <div class="eq-formula">
          $$a_i = \exp\left(\sum_j (1 - X_{ij}) \log(1 - m_j)\right)$$
        </div>
        <div class="eq-desc">
          Or work directly in log-space during optimisation.
        </div>
      </div>
    </section>

    <!-- SECTION 6: LOSS FUNCTION -->
    <section id="loss" class="reveal">
      <div class="section-header">
        <div class="section-icon">üìâ</div>
        <h2>6. Loss function</h2>
      </div>

      <div class="eq-item">
        <div class="eq-header">
          <span class="eq-title">Activation objectives</span>
        </div>
        <div class="eq-formula">
          $$\text{Lesion: } a_i \to 1 \quad\quad \text{Healthy: } a_i \to 0$$
        </div>
      </div>

      <div class="eq-item" style="margin-top:8px;">
        <div class="eq-header">
          <span class="eq-title">Loss formulation</span>
          <span class="eq-tag">Differentiable</span>
        </div>
        <div class="eq-formula">
          $$\mathcal{L}_{\text{activation}} = -\mathbb{E}_{i \in \text{lesion}}[\log(a_i + \epsilon)] + \mathbb{E}_{i \in \text{healthy}}[a_i]$$
        </div>
        <div class="eq-desc">
          <ul style="margin-left:20px;">
            <li>First term: maximises activation in lesion cells (via negative log-likelihood)</li>
            <li>Second term: directly penalises activation in healthy cells</li>
            <li>$\epsilon$ prevents $\log(0)$</li>
          </ul>
        </div>
      </div>

      <div class="eq-item" style="margin-top:8px;">
        <div class="eq-header">
          <span class="eq-title">Alternative form (squared error)</span>
        </div>
        <div class="eq-formula">
          $$\mathcal{L}_{\text{activation}} = \frac{1}{N}\sum_i \left[ y_i(1-a_i)^2 + (1-y_i)a_i^2 \right]$$
        </div>
        <div class="eq-desc">
          Where $y_i = 1$ for lesion cells, $0$ for healthy cells.
        </div>
      </div>
    </section>

    <!-- SECTION 7: CARDINALITY CONSTRAINT -->
    <section id="cardinality" class="reveal">
      <div class="section-header">
        <div class="section-icon">2Ô∏è‚É£</div>
        <h2>7. Enforcing dual targeting</h2>
      </div>

      <div class="eq-item">
        <div class="eq-header">
          <span class="eq-title">Cardinality penalty</span>
          <span class="eq-tag">Exactly 2 markers</span>
        </div>
        <div class="eq-formula">
          $$\mathcal{L}_{\text{card}} = \lambda \left( \sum_j m_j - 2 \right)^2$$
        </div>
        <div class="eq-desc">
          This penalises deviations from selecting exactly two markers. $\lambda$ controls the strength of the constraint.
        </div>
      </div>

      <div class="eq-item" style="margin-top:8px;">
        <div class="eq-header">
          <span class="eq-title">Total loss</span>
          <span class="eq-tag">End-to-end differentiable</span>
        </div>
        <div class="eq-formula">
          $$\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{activation}} + \mathcal{L}_{\text{card}}$$
        </div>
        <div class="eq-desc">
          The entire loss is differentiable with respect to the selection logits $\alpha_j$. We optimise using gradient descent (Adam) with temperature annealing.
        </div>
      </div>
    </section>

    <!-- SECTION 8: HARD INFERENCE -->
    <section id="inference" class="reveal">
      <div class="section-header">
        <div class="section-icon">üéØ</div>
        <h2>8. Hard inference</h2>
      </div>

      <div class="eq-item">
        <div class="eq-header">
          <span class="eq-title">Final discrete selection</span>
        </div>
        <div class="eq-formula">
          $$\text{Take top 2 genes by } \alpha_j$$
        </div>
        <div class="eq-desc">
          After training with low temperature ($\tau \to 0$), the soft selections $m_j$ become nearly binary. The final hard selection is simply the two genes with the highest logits $\alpha_j$.
        </div>
      </div>

      <div class="callout" style="margin-top:20px; border-left-color: var(--accent);">
        <p><strong>What we've built:</strong> This is no longer a classifier, a representation learner, or a sparse autoencoder in the usual sense. This is <strong>differentiable Boolean subset optimisation over binary gene expression</strong> ‚Äî a principled way to find the optimal marker pair without brute-force enumeration.</p>
      </div>
    </section>

    <!-- SUMMARY TABLE -->
    <section class="reveal" style="margin-top:24px;">
      <div class="section-header">
        <div class="section-icon">üìã</div>
        <h2>Summary of notation</h2>
      </div>

      <div style="background: var(--surface); border: 1px solid var(--border); border-radius: 10px; overflow: hidden;">
        <table style="width:100%; border-collapse: collapse; font-size: 12px; font-family: 'DM Mono', monospace;">
          <thead style="background: var(--surface2); border-bottom: 1px solid var(--border);">
            <tr><th style="padding: 12px; text-align: left;">Symbol</th><th style="padding: 12px; text-align: left;">Meaning</th></tr>
          </thead>
          <tbody>
            <tr><td style="padding: 8px 12px; border-bottom: 1px solid var(--border);">$X$</td><td style="padding: 8px 12px; border-bottom: 1px solid var(--border);">Binary expression matrix (cells √ó genes)</td></tr>
            <tr><td style="padding: 8px 12px; border-bottom: 1px solid var(--border);">$\alpha_j$</td><td style="padding: 8px 12px; border-bottom: 1px solid var(--border);">Learnable logit for gene $j$</td></tr>
            <tr><td style="padding: 8px 12px; border-bottom: 1px solid var(--border);">$m_j$</td><td style="padding: 8px 12px; border-bottom: 1px solid var(--border);">Soft selection weight ($m_j = \sigma((\alpha_j + g_j)/\tau)$)</td></tr>
            <tr><td style="padding: 8px 12px; border-bottom: 1px solid var(--border);">$a_i$</td><td style="padding: 8px 12px; border-bottom: 1px solid var(--border);">Activation probability for cell $i$</td></tr>
            <tr><td style="padding: 8px 12px; border-bottom: 1px solid var(--border);">$\tau$</td><td style="padding: 8px 12px; border-bottom: 1px solid var(--border);">Temperature (annealed during training)</td></tr>
            <tr><td style="padding: 8px 12px; border-bottom: 1px solid var(--border);">$g_j$</td><td style="padding: 8px 12px; border-bottom: 1px solid var(--border);">Gumbel noise for stochastic relaxation</td></tr>
            <tr><td style="padding: 8px 12px;">$\lambda$</td><td style="padding: 8px 12px;">Cardinality penalty weight</td></tr>
          </tbody>
        </table>
      </div>
    </section>

    <div class="footer">
      <span>bloomsbury burger therapeutics plc ¬© 2026</span>
      <span>pre-seed ¬∑ pre-revenue ¬∑ pre-sanity</span>
    </div>

  </main>
</div>

<script src="script.js"></script>
</body>
</html>
